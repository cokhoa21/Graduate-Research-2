# -*- coding: utf-8 -*-
"""Cookie_Classifier_Word_level.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D-zQl8ITzx6Bf-5n22Sc6WzjHBjoczaB
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pip install -U keras
!pip install -U torchtext==0.18.0
!pip install -U torch==2.3.0
!pip install tensorflow

import os
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import pandas as pd
import re
import torchtext
torchtext.disable_torchtext_deprecation_warning()
from torchtext.data.functional import to_map_style_dataset
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import WeightedRandomSampler
from torch.utils.data import DataLoader
import time
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score
from itertools import cycle
import seaborn as sns
import torch.optim as optim
import logging
from datetime import datetime
import os

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# # Đường dẫn tới thư mục trong Google Drive để lưu các file
save_path = '/content/gdrive/MyDrive/Cookie_Classifier'
os.makedirs(save_path, exist_ok=True)  # Tạo thư mục nếu chưa tồn tại

# # Đọc file CSV
file_path = '/content/gdrive/MyDrive/Cookie_Dataset_Test/data.csv'
df = pd.read_csv(file_path)

# # Chỉ lấy n hàng đầu tiên
# df = df.head(500000)

# # # Chia dataset thành 2 tập: 80:20
train_size = int(0.8 * len(df))
val_size = int(0.2 * len(df))

train_data = df[:train_size]
val_data = df[train_size:train_size + val_size]

# Lưu 3 tập vào Google Drive
train_data.to_csv(os.path.join(save_path, 'train.csv'), index=False)
val_data.to_csv(os.path.join(save_path, 'valid.csv'), index=False)

print("Train, validation, and test datasets have been saved to:", save_path)

def load_data_from_path(file_path):
    examples = []
    df = pd.read_csv(file_path)

    # Giữ lại các hàng có label thuộc {0, 1, 2, 3, 4}
    df = df[df['label'].isin({0, 1, 2, 3, 4})]

    for _, row in df.iterrows():
        text = row['value']
        label = row['label']
        data = {
            'sentence': text,
            'label': label
        }
        examples.append(data)

    return pd.DataFrame(examples)

file_paths = {
    'train': '/content/gdrive/MyDrive/Cookie_Classifier/train.csv',
    'valid': '/content/gdrive/MyDrive/Cookie_Classifier/valid.csv',
}

train_df = load_data_from_path(file_paths['train'])
valid_df = load_data_from_path(file_paths['valid'])

train_df

def preprocess_text(text):
  if isinstance(text, str):
    text = text.lower()
  return text

train_df['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in train_df.iterrows()]
valid_df['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in valid_df.iterrows()]

train_df = train_df.dropna(subset=['preprocess_sentence'])
valid_df = valid_df.dropna(subset=['preprocess_sentence'])

def create_word_vocab(sentences, max_words=10000):
    """
    Tạo từ điển word-level từ tập dữ liệu văn bản

    Args:
        sentences: Chuỗi, danh sách chuỗi, hoặc pandas.Series chứa các câu
        max_words: Số lượng từ tối đa sẽ giữ lại

    Returns:
        tokenizer: Đối tượng Tokenizer đã được huấn luyện
    """
    if isinstance(sentences, str):
        sentences = [sentences]
    elif isinstance(sentences, pd.Series):
        # Áp dụng hàm làm sạch cho mỗi câu
        sentences = sentences.dropna().apply(preprocess_text).tolist()
    elif isinstance(sentences, list):
        # Áp dụng hàm làm sạch cho mỗi câu trong danh sách
        sentences = [preprocess_text(s) for s in sentences if isinstance(s, str)]
    else:
        raise ValueError("Đầu vào phải là một chuỗi, danh sách, hoặc pandas.Series")

    # Tạo tokenizer
    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')
    tokenizer.fit_on_texts(sentences)

    return tokenizer

def word_encode(sentences, tokenizer, maxlen=50):
    """
    Mã hóa câu thành chuỗi số sử dụng word-level encoding

    Args:
        sentences: Chuỗi, danh sách chuỗi, hoặc pandas.Series chứa các câu
        tokenizer: Đối tượng Tokenizer đã được huấn luyện
        maxlen: Độ dài tối đa của chuỗi sau khi mã hóa

    Returns:
        padded_sequences: Mảng numpy của các chuỗi đã được mã hóa và đệm
    """
    if isinstance(sentences, str):
        sentences = [sentences]
    elif isinstance(sentences, pd.Series):
        # Áp dụng hàm làm sạch cho mỗi câu
        sentences = sentences.dropna().apply(preprocess_text).tolist()
    elif isinstance(sentences, list):
        # Áp dụng hàm làm sạch cho mỗi câu trong danh sách
        sentences = [preprocess_text(s) for s in sentences if isinstance(s, str)]
    else:
        raise ValueError("Đầu vào phải là một chuỗi, danh sách, hoặc pandas.Series")

    # Chuyển đổi thành chuỗi số
    sequences = tokenizer.texts_to_sequences(sentences)

    # Đệm chuỗi để có cùng độ dài
    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')

    return padded_sequences

# Sử dụng
# Giả sử train_df là DataFrame của bạn với cột 'preprocess_sentence'

# Bước 1: Tạo tokenizer từ dữ liệu huấn luyện
tokenizer = create_word_vocab(train_df['preprocess_sentence'], max_words=100000)

# Bước 2: Mã hóa dữ liệu thành chuỗi số
word_sequences = word_encode(train_df['preprocess_sentence'], tokenizer, maxlen=50)

print(f"Kích thước của chuỗi đã mã hóa: {word_sequences.shape}")
print(f"Mẫu chuỗi đã mã hóa: {word_sequences[1]}")

# Thông tin thêm về từ điển
word_index = tokenizer.word_index
print(f"Kích thước từ điển: {len(word_index)}")
print(f"Một số từ đầu tiên: {list(word_index.items())[:10]}")

# Lưu tokenizer để sử dụng sau này
import pickle
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)

def calculate_maxlen(df, tokenizer):
    """
    Tính toán độ dài tối đa của câu sau khi đã tokenize và in ra câu dài nhất

    Args:
        df: DataFrame chứa dữ liệu
        tokenizer: Đối tượng Tokenizer đã được huấn luyện

    Returns:
        maxlen: Độ dài tối đa của câu sau khi mã hóa
    """
    max_length = 0
    longest_sentence = ""
    longest_encoded = []

    for index, row in df.iterrows():
        sentence = row['preprocess_sentence']

        # Bỏ qua các câu None
        if not isinstance(sentence, str):
            continue

        # Chuyển câu thành chuỗi các số sử dụng tokenizer
        encoded_sentence = tokenizer.texts_to_sequences([sentence])[0]

        # Kiểm tra nếu câu này dài hơn câu dài nhất hiện tại
        if len(encoded_sentence) > max_length:
            max_length = len(encoded_sentence)
            longest_sentence = sentence
            longest_encoded = encoded_sentence

    # Đảm bảo maxlen không quá nhỏ
    max_length = max(max_length, 10)  # Ít nhất 10 token

    print(f"Calculated maxlen: {max_length}")
    print(f"\nLongest sentence (length={max_length}):")

    # In ra câu dài nhất (cắt ngắn nếu quá dài để dễ đọc)
    if len(longest_sentence) > 1000:
        print(f"{longest_sentence[:1000]}... (truncated)")
    else:
        print(longest_sentence)

    # In ra một số token đầu tiên của câu dài nhất sau khi đã mã hóa
    print(f"\nFirst 20 tokens of encoded longest sentence:")
    print(longest_encoded[:20])

    # In ra từ tương ứng với các token (giải mã)
    idx_to_word = {v: k for k, v in tokenizer.word_index.items()}
    idx_to_word[0] = '<PAD>'  # Thêm token PAD

    decoded_tokens = [idx_to_word.get(idx, '<UNK>') for idx in longest_encoded[:20]]
    print(f"\nFirst 20 decoded tokens:")
    print(decoded_tokens)

    return max_length

def prepare_dataset_word_level(df, tokenizer, maxlen):
    """
    Chuẩn bị dataset với word-level encoding

    Args:
        df: DataFrame chứa dữ liệu
        tokenizer: Đối tượng Tokenizer đã được huấn luyện
        maxlen: Độ dài tối đa của chuỗi sau khi mã hóa

    Returns:
        Iterator cung cấp (encoded_sentence, label)
    """
    # create iterator for dataset: (sentence, label)
    for index, row in df.iterrows():
        sentence = row['preprocess_sentence']

        # Bỏ qua các câu None
        if not isinstance(sentence, str):
            continue

        # Chuyển câu thành chuỗi các số sử dụng tokenizer
        encoded_sentence = tokenizer.texts_to_sequences([sentence])[0]

        # Thực hiện padding hoặc truncate để đạt được độ dài cố định
        if len(encoded_sentence) < maxlen:
            encoded_sentence = encoded_sentence + [0] * (maxlen - len(encoded_sentence))
        else:
            encoded_sentence = encoded_sentence[:maxlen]

        label = row['label']
        yield encoded_sentence, label

maxlen = calculate_maxlen(train_df, tokenizer)

# Sử dụng tokenizer đã được tạo trước đó
# tokenizer = create_word_vocab(train_df['preprocess_sentence'], max_words=vocab_size)

# Chuẩn bị các dataset
train_dataset = prepare_dataset_word_level(train_df, tokenizer, maxlen)
train_dataset = to_map_style_dataset(train_dataset)

valid_dataset = prepare_dataset_word_level(valid_df, tokenizer, maxlen)
valid_dataset = to_map_style_dataset(valid_dataset)

def collate_batch(batch):
    """
    Tạo batch từ danh sách các mẫu

    Args:
        batch: Danh sách các cặp (encoded_sentence, label)

    Returns:
        Tensor các câu đã mã hóa và tensor các nhãn
    """
    # Tách câu và nhãn
    sentences, labels = list(zip(*batch))

    # Chuyển đổi thành tensor
    encoded_sentences = torch.tensor(sentences, dtype=torch.int64)
    labels = torch.tensor(labels)

    return encoded_sentences, labels

y_train = torch.tensor([label for _, label in train_dataset], dtype=torch.long)

class_counts = [(y_train == i).sum().item() for i in range(5)]
class_weights = [1.0 / count if count else 0.0 for count in class_counts] # Handle zero counts
sample_weights = [class_weights[label] for label in y_train]

sampler = WeightedRandomSampler(sample_weights, len(sample_weights))

batch_size = 1024

train_dataloader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    collate_fn=collate_batch,
    sampler=sampler
)
valid_dataloader = DataLoader(
    valid_dataset,
    batch_size=batch_size,
    shuffle=False,
    collate_fn=collate_batch
)

next(iter(train_dataloader))

encoded_sentences, labels = next(iter(train_dataloader))

encoded_sentences.shape

labels.shape

def train_epoch(model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50):
    model.train()
    total_acc, total_count = 0, 0
    losses = []
    start_time = time.time()

    for idx, (inputs, labels) in enumerate(train_dataloader):
        inputs = inputs.to(device)
        labels = labels.type(torch.LongTensor).to(device) # This line is modified

        optimizer.zero_grad()

        predictions = model(inputs, device)

        # compute loss
        loss = criterion(predictions, labels)
        losses.append(loss.item())

        # backward
        loss.backward()
        optimizer.step()
        total_acc += (predictions.argmax(1) == labels).sum().item()
        total_count += labels.size(0)
        if idx % log_interval == 0 and idx > 0:
            elapsed = time.time() - start_time
            print(
                "| epoch {:3d} | {:5d}/{:5d} batches "
                "| accuracy {:8.3f}".format(
                    epoch, idx, len(train_dataloader), total_acc / total_count
                )
            )
            total_acc, total_count = 0, 0
            start_time = time.time()

    epoch_acc = total_acc / total_count
    epoch_loss = sum(losses) / len(losses)
    return epoch_acc, epoch_loss

def evaluate_epoch(model, criterion, valid_dataloader, device):
    model.eval()
    total_acc, total_count = 0, 0
    losses = []

    with torch.no_grad():
        for idx, (inputs, labels) in enumerate(valid_dataloader):
            inputs = inputs.to(device)
            labels = labels.type(torch.LongTensor).to(device) # This line is modified

            predictions = model(inputs, device)

            loss = criterion(predictions, labels)
            losses.append(loss.item())

            total_acc += (predictions.argmax(1) == labels).sum().item()
            total_count += labels.size(0)

    epoch_acc = total_acc / total_count
    epoch_loss = sum(losses) / len(losses)
    return epoch_acc, epoch_loss

def train(model, model_name, save_model, optimizer, criterion, train_dataloader, valid_dataloader, num_epochs, device):
    train_accs, train_losses = [], []
    eval_accs, eval_losses = [], []
    best_loss_eval = 100
    times = []
    for epoch in range(1, num_epochs+1):
        epoch_start_time = time.time()
        # Training
        train_acc, train_loss = train_epoch(model, optimizer, criterion, train_dataloader, device, epoch)
        train_accs.append(train_acc)
        train_losses.append(train_loss)

        # Evaluation
        eval_acc, eval_loss = evaluate_epoch(model, criterion, valid_dataloader, device)
        eval_accs.append(eval_acc)
        eval_losses.append(eval_loss)

        # Save best model
        if eval_loss < best_loss_eval:
            torch.save(model.state_dict(), save_model + f'/{model_name}.pt')

        times.append(time.time() - epoch_start_time)
        # Print loss, acc end epoch
        print("-" * 59)
        print(
            "| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f} "
            "| Valid Accuracy {:8.3f} | Valid Loss {:8.3f} ".format(
                epoch, time.time() - epoch_start_time, train_acc, train_loss, eval_acc, eval_loss
            )
        )
        print("-" * 59)

    # Load best model
    model.load_state_dict(torch.load(save_model + f'/{model_name}.pt'))
    model.eval()
    metrics = {
        'train_accuracy': train_accs,
        'train_loss': train_losses,
        'valid_accuracy': eval_accs,
        'valid_loss': eval_losses,
        'time': times
    }
    return model, metrics

import matplotlib.pyplot as plt

def plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses):
    epochs = list(range(num_epochs))
    fig, axs = plt.subplots(nrows = 1, ncols =2 , figsize = (12,6))
    axs[0].plot(epochs, train_accs, label = "Training")
    axs[0].plot(epochs, eval_accs, label = "Evaluation")
    axs[1].plot(epochs, train_losses, label = "Training")
    axs[1].plot(epochs, eval_losses, label = "Evaluation")
    axs[0].set_xlabel("Epochs")
    axs[1].set_xlabel("Epochs")
    axs[0].set_ylabel("Accuracy")
    axs[1].set_ylabel("Loss")
    plt.legend()

class TokenAndPositionEmbedding(nn.Module):
    def __init__(self, vocab_size, embed_dim, max_length, device='cpu'):
        super().__init__()
        self.device = device
        self.word_emb = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim
        )
        self.pos_emb = nn.Embedding(
            num_embeddings=max_length,
            embedding_dim=embed_dim
        )

    def forward(self, x):
        N, seq_len = x.size()
        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)
        output1 = self.word_emb(x)
        output2 = self.pos_emb(positions)
        output = output1 + output2
        return output

embed_dim = 128
vocab_size = 100000
max_length = maxlen
embedding = TokenAndPositionEmbedding(
    vocab_size,
    embed_dim,
    max_length
)

class TransformerEncoder(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.attn = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            batch_first=True
        )
        self.ffn = nn.Sequential(
            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),
            nn.ReLU(),
            nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)
        )
        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)
        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)
        self.dropout_1 = nn.Dropout(p=dropout)
        self.dropout_2 = nn.Dropout(p=dropout)

    def forward(self, query, key, value):
        attn_output, _ = self.attn(query, key, value)
        attn_output = self.dropout_1(attn_output)
        out_1 = self.layernorm_1(query + attn_output)
        ffn_output = self.ffn(out_1)
        ffn_output = self.dropout_2(ffn_output)
        out_2 = self.layernorm_2(out_1 + ffn_output)
        return out_2

class TransformerEncoderCls(nn.Module):
    def __init__(self,
                 vocab_size, max_length, embed_dim, num_heads, ff_dim,
                 dropout=0.1, device='cpu'
        ):
        super().__init__()
        self.embd_layer = TokenAndPositionEmbedding(
            vocab_size, embed_dim, max_length, device
        )
        self.transformer_layer = TransformerEncoder(
            embed_dim, num_heads, ff_dim, dropout
        )
        self.pooling = nn.AvgPool1d(kernel_size=max_length)
        self.fc1 = nn.Linear(in_features=embed_dim, out_features=20)
        self.fc2 = nn.Linear(in_features=20, out_features=5)
        self.dropout = nn.Dropout(p=dropout)
        self.relu = nn.ReLU()
    def forward(self, x, device):
        output = self.embd_layer(x)
        output = self.transformer_layer(output, output, output)
        output = self.pooling(output.permute(0,2,1)).squeeze()
        output = self.dropout(output)
        output = self.fc1(output)
        output = self.dropout(output)
        output = self.fc2(output)
        return output

encoded_sentences.shape

def setup_logger(log_path):
    """
    Thiết lập logger để ghi log theo định dạng yêu cầu

    Args:
        log_path: Đường dẫn của file log

    Returns:
        logger: Đối tượng logger đã được cấu hình
    """
    # Tạo logger
    logger = logging.getLogger('classifier')
    logger.setLevel(logging.INFO)

    # Tạo file handler
    fh = logging.FileHandler(log_path)
    fh.setLevel(logging.INFO)

    # Tạo formatter
    formatter = logging.Formatter('%(asctime)s :: %(name)s :: %(levelname)s :: %(message)s',
                                 datefmt='%Y-%m-%d-%H:%M:%S')

    # Thêm formatter vào handler
    fh.setFormatter(formatter)

    # Thêm handler vào logger
    logger.handlers = []  # Xóa handlers cũ (nếu có)
    logger.addHandler(fh)

    return logger

def log_metrics(true_labels, pred_labels, log_path, total_samples):
    """
    Ghi log các metrics theo định dạng yêu cầu

    Args:
        true_labels: Nhãn thực tế
        pred_labels: Nhãn dự đoán
        log_path: Đường dẫn để lưu file log
        total_samples: Tổng số mẫu
    """
    # Thiết lập logger
    logger = setup_logger(log_path)

    # Tính toán accuracy
    accuracy = accuracy_score(true_labels, pred_labels)
    correct_count = int(accuracy * total_samples)

    # Tính toán precision, recall, f1 cho mỗi lớp
    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average=None
    )

    # Tính toán micro average
    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average='micro'
    )

    # Tính toán macro average
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average='macro'
    )

    # Tính toán weighted average
    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average='weighted'
    )

    # Ghi log theo định dạng yêu cầu
    logger.info(f"Total Accuracy Count: {correct_count}")
    logger.info(f"Total Accuracy Ratio: {accuracy}")
    logger.info(f"Micro Precision: {precision_micro}")
    logger.info(f"Micro Recall: {recall_micro}")
    logger.info(f"Micro F1Score: {f1_micro}")
    logger.info(f"Macro Precision: {precision_macro}")
    logger.info(f"Macro Recall: {recall_macro}")
    logger.info(f"Macro F1Score: {f1_macro}")
    logger.info(f"Weighted Precision: {precision_weighted}")
    logger.info(f"Weighted Recall: {recall_weighted}")
    logger.info(f"Weighted F1Score: {f1_weighted}")
    logger.info(f"Precision for each class: {precision_per_class}")
    logger.info(f"Recall for each class: {recall_per_class}")
    logger.info(f"F1Score for each class: {f1_per_class}")
    logger.info(f"-------------------------------")
    logger.info(f"(Old Method) Total Accuracy: {accuracy*100:.3f}%")
    logger.info(f"(Old Method) Precision: {precision_per_class}")
    logger.info(f"(Old Method) Recall: {recall_per_class}")
    logger.info(f"(Old Method) F1 Scores: {f1_per_class}")

    return {
        'accuracy': accuracy,
        'precision_per_class': precision_per_class,
        'recall_per_class': recall_per_class,
        'f1_per_class': f1_per_class,
        'precision_micro': precision_micro,
        'recall_micro': recall_micro,
        'f1_micro': f1_micro,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'precision_weighted': precision_weighted,
        'recall_weighted': recall_weighted,
        'f1_weighted': f1_weighted
    }

def evaluate_model(model, dataloader, criterion, device):
    """
    Evaluates the model with comprehensive metrics:
    - Accuracy
    - Precision, Recall, F1-score for each class
    - Confusion Matrix
    - Classification Report

    Args:
        model: Trained model
        dataloader: DataLoader for evaluation data
        criterion: Loss function
        device: Device to run evaluation on (cuda/cpu)

    Returns:
        dict: Dictionary containing all evaluation metrics
    """
    model.eval()
    total_acc, total_count = 0, 0
    all_predictions = []
    all_labels = []
    losses = []

    with torch.no_grad():
        for idx, (inputs, labels) in enumerate(dataloader):
            inputs = inputs.to(device)
            labels = labels.type(torch.LongTensor).to(device)

            predictions = model(inputs, device)
            pred_labels = predictions.argmax(1)

            # Store predictions and true labels
            all_predictions.extend(pred_labels.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            # Calculate accuracy
            total_acc += (pred_labels == labels).sum().item()
            total_count += labels.size(0)

            # Calculate loss
            loss = criterion(predictions, labels)
            losses.append(loss.item())

    # Convert lists to numpy arrays
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)

    # Calculate metrics
    precision, recall, f1, support = precision_recall_fscore_support(
        all_labels,
        all_predictions,
        average=None
    )

    # Calculate macro and weighted averages
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
        all_labels,
        all_predictions,
        average='macro'
    )

    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
        all_labels,
        all_predictions,
        average='weighted'
    )

    # Create classification report
    class_report = classification_report(
        all_labels,
        all_predictions,
        output_dict=True
    )

    # Create confusion matrix
    conf_matrix = confusion_matrix(all_labels, all_predictions)

    # Calculate overall metrics
    accuracy = total_acc / total_count
    avg_loss = sum(losses) / len(losses)

    # Store all metrics in a dictionary
    metrics = {
        'accuracy': accuracy,
        'loss': avg_loss,
        'precision_per_class': precision,
        'recall_per_class': recall,
        'f1_per_class': f1,
        'support_per_class': support,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'precision_weighted': precision_weighted,
        'recall_weighted': recall_weighted,
        'f1_weighted': f1_weighted,
        'classification_report': class_report,
        'confusion_matrix': conf_matrix
    }

    return metrics


def visualize_metrics(metrics):
    """
    Visualizes evaluation metrics with plots.

    Args:
        metrics: Dictionary containing evaluation metrics
    """
    # Create figure with subplots
    fig, axs = plt.subplots(2, 2, figsize=(15, 12))

    # Plot precision, recall, and F1 per class
    classes = range(len(metrics['precision_per_class']))

    axs[0, 0].bar(classes, metrics['precision_per_class'], alpha=0.7, label='Precision')
    axs[0, 0].bar(classes, metrics['recall_per_class'], alpha=0.5, label='Recall')
    axs[0, 0].bar(classes, metrics['f1_per_class'], alpha=0.3, label='F1')
    axs[0, 0].set_xticks(classes)
    axs[0, 0].set_xlabel('Class')
    axs[0, 0].set_ylabel('Score')
    axs[0, 0].set_title('Precision, Recall, and F1 per Class')
    axs[0, 0].legend()

    # Plot confusion matrix
    conf_matrix = metrics['confusion_matrix']
    sns.heatmap(
        conf_matrix,
        annot=True,
        fmt='d',
        cmap='Blues',
        ax=axs[0, 1]
    )
    axs[0, 1].set_xlabel('Predicted Label')
    axs[0, 1].set_ylabel('True Label')
    axs[0, 1].set_title('Confusion Matrix')

    # Plot macro vs weighted metrics
    metric_names = ['Precision', 'Recall', 'F1-Score']
    macro_metrics = [
        metrics['precision_macro'],
        metrics['recall_macro'],
        metrics['f1_macro']
    ]
    weighted_metrics = [
        metrics['precision_weighted'],
        metrics['recall_weighted'],
        metrics['f1_weighted']
    ]

    axs[1, 0].bar(metric_names, macro_metrics, alpha=0.7, label='Macro')
    axs[1, 0].bar(metric_names, weighted_metrics, alpha=0.5, label='Weighted')
    axs[1, 0].set_ylim(0, 1)
    axs[1, 0].set_title('Macro vs Weighted Metrics')
    axs[1, 0].legend()

    # Display summary with text
    axs[1, 1].axis('off')
    summary_text = f"""
    Summary Metrics:

    Accuracy: {metrics['accuracy']:.4f}
    Loss: {metrics['loss']:.4f}

    Macro-Average:
      Precision: {metrics['precision_macro']:.4f}
      Recall: {metrics['recall_macro']:.4f}
      F1-Score: {metrics['f1_macro']:.4f}

    Weighted-Average:
      Precision: {metrics['precision_weighted']:.4f}
      Recall: {metrics['recall_weighted']:.4f}
      F1-Score: {metrics['f1_weighted']:.4f}
    """
    axs[1, 1].text(0, 0.5, summary_text, fontsize=12)

    plt.tight_layout()
    plt.show()

def print_classification_report(metrics):
    """
    Prints a formatted classification report.

    Args:
        metrics: Dictionary containing evaluation metrics
    """
    report = metrics['classification_report']

    # Print header
    print("\nClassification Report:")
    print("-" * 80)
    print(f"{'Class':^10} | {'Precision':^10} | {'Recall':^10} | {'F1-Score':^10} | {'Support':^10}")
    print("-" * 80)

    # Print per-class metrics
    for cls in sorted([c for c in report.keys() if c.isdigit()]):
        # Use 'f' format for support as it might be a float
        print(f"{int(cls):^10d} | {report[cls]['precision']:^10.4f} | {report[cls]['recall']:^10.4f} | {report[cls]['f1-score']:^10.4f} | {report[cls]['support']:^10.0f}")

    # Print averages
    print("-" * 80)
    print(f"{'macro avg':^10} | {report['macro avg']['precision']:^10.4f} | {report['macro avg']['recall']:^10.4f} | {report['macro avg']['f1-score']:^10.4f} | {report['macro avg']['support']:^10.0f}") # Also change format for macro avg support
    print(f"{'weighted avg':^10} | {report['weighted avg']['precision']:^10.4f} | {report['weighted avg']['recall']:^10.4f} | {report['weighted avg']['f1-score']:^10.4f} | {report['weighted avg']['support']:^10.0f}") # Also change format for weighted avg support
    print("-" * 80)
    print(f"Accuracy: {report['accuracy']:.4f}")
    print("-" * 80)


# Example of how to use these functions in the model testing phase:
def evaluate_final_model(model, test_dataloader, criterion, device):
    """
    Performs a comprehensive evaluation of the final model.

    Args:
        model: Trained model
        test_dataloader: DataLoader for test data
        criterion: Loss function
        device: Device to run evaluation on (cuda/cpu)
    """
    print("Performing comprehensive evaluation of the model...")

    # Get all metrics
    metrics = evaluate_model(model, test_dataloader, criterion, device)

    # Print accuracy and loss
    print(f"\nTest Accuracy: {metrics['accuracy']:.4f}")
    print(f"Test Loss: {metrics['loss']:.4f}")

    # Print detailed classification report
    print_classification_report(metrics)

    # Visualize metrics
    visualize_metrics(metrics)

    return metrics

def perform_cross_validation(train_df, n_folds=5, num_epochs=5, batch_size=1024, save_model_dir='./model'):
    """
    Thực hiện k-fold cross validation đúng cách, chỉ sử dụng tập train và bảo đảm không bị data leakage.

    Args:
        train_df: DataFrame chứa dữ liệu huấn luyện (chỉ tập train, không bao gồm validation hay test)
        n_folds: Số fold cho cross validation
        num_epochs: Số epoch cho mỗi fold
        batch_size: Kích thước batch
        save_model_dir: Thư mục lưu mô hình

    Returns:
        dict: Dictionary chứa kết quả trung bình và metrics cho từng fold
    """
    print("\nLƯU Ý: Cross-validation chỉ sử dụng tập train, tập test được giữ riêng cho đánh giá cuối cùng.")

    # Tạo KFold
    from sklearn.model_selection import KFold
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

    # Chuẩn bị các metrics để lưu kết quả
    cv_accuracies = []
    cv_losses = []
    cv_f1_weighted = []
    cv_f1_macro = []
    cv_metrics_per_fold = []

    # Duyệt qua từng fold
    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):
        print(f"\n{'='*20} Fold {fold+1}/{n_folds} {'='*20}")

        # Chia dữ liệu thành train và validation cho fold hiện tại
        fold_train_df = train_df.iloc[train_idx].reset_index(drop=True)
        fold_val_df = train_df.iloc[val_idx].reset_index(drop=True)

        print(f"Train size: {len(fold_train_df)}, Validation size: {len(fold_val_df)}")

        # QUAN TRỌNG: Xây dựng tokenizer CHỈ trên tập huấn luyện của fold hiện tại
        print("Building vocabulary only on the training fold...")
        fold_tokenizer = create_word_vocab(fold_train_df['preprocess_sentence'], max_words=100000)
        vocab_size = len(fold_tokenizer.word_index) + 1  # +1 cho token padding
        print(f"Vocabulary size for fold {fold+1}: {vocab_size}")

        # Tính toán maxlen dựa trên tập huấn luyện của fold hiện tại
        fold_maxlen = calculate_maxlen(fold_train_df, fold_tokenizer)

        # Chuẩn bị dataset sử dụng tokenizer và maxlen của fold hiện tại
        fold_train_dataset = prepare_dataset_word_level(fold_train_df, fold_tokenizer, fold_maxlen)
        fold_train_dataset = to_map_style_dataset(fold_train_dataset)

        fold_val_dataset = prepare_dataset_word_level(fold_val_df, fold_tokenizer, fold_maxlen)
        fold_val_dataset = to_map_style_dataset(fold_val_dataset)

        # Tạo weighted sampler để xử lý dữ liệu không cân bằng
        y_train = torch.tensor([label for _, label in fold_train_dataset], dtype=torch.long)
        class_counts = [(y_train == i).sum().item() for i in range(5)]
        class_weights = [1.0 / count if count else 0.0 for count in class_counts]
        sample_weights = [class_weights[label] for label in y_train]
        sampler = WeightedRandomSampler(sample_weights, len(sample_weights))

        # Tạo dataloader
        fold_train_dataloader = DataLoader(
            fold_train_dataset,
            batch_size=batch_size,
            collate_fn=collate_batch,
            sampler=sampler
        )

        fold_val_dataloader = DataLoader(
            fold_val_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_batch
        )

        # QUAN TRỌNG: Khởi tạo model MỚI cho mỗi fold
        print(f"Initializing a new model for fold {fold+1}...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Cập nhật hyperparameters dựa trên dữ liệu của fold
        embed_dim = 128
        num_heads = 4
        ff_dim = 256
        dropout = 0.1

        model = TransformerEncoderCls(
            vocab_size, fold_maxlen, embed_dim, num_heads, ff_dim, dropout, device
        )
        model.to(device)

        # Khởi tạo optimizer và loss function
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.00005)

        # Tạo thư mục lưu model cho fold hiện tại
        fold_save_dir = os.path.join(save_model_dir, f'fold_{fold+1}')
        os.makedirs(fold_save_dir, exist_ok=True)

        # Huấn luyện model
        model, fold_metrics = train(
            model, f'model_fold_{fold+1}', fold_save_dir, optimizer, criterion,
            fold_train_dataloader, fold_val_dataloader, num_epochs, device
        )

        # Đánh giá model trên tập validation
        val_metrics = evaluate_model(model, fold_val_dataloader, criterion, device)

        # Lưu các metrics
        cv_accuracies.append(val_metrics['accuracy'])
        cv_losses.append(val_metrics['loss'])
        cv_f1_weighted.append(val_metrics['f1_weighted'])
        cv_f1_macro.append(val_metrics['f1_macro'])
        cv_metrics_per_fold.append(val_metrics)

        # In kết quả cho fold hiện tại
        print(f"\nFold {fold+1} Results:")
        print(f"Accuracy: {val_metrics['accuracy']:.4f}")
        print(f"Loss: {val_metrics['loss']:.4f}")
        print(f"F1 Weighted: {val_metrics['f1_weighted']:.4f}")
        print(f"F1 Macro: {val_metrics['f1_macro']:.4f}")

        # Thu thập true labels và pred labels cho logging
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for inputs, labels in fold_val_dataloader:
                inputs = inputs.to(device)
                labels = labels.type(torch.LongTensor).to(device)
                predictions = model(inputs, device)
                preds = predictions.argmax(1).cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(labels.cpu().numpy())

        # Ghi log metrics theo định dạng yêu cầu
        log_file_path = os.path.join(fold_save_dir, f'metrics_fold_{fold+1}.log')
        detailed_metrics = log_metrics(all_labels, all_preds, log_file_path, len(all_labels))

        # Vẽ confusion matrix cho fold hiện tại
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            val_metrics['confusion_matrix'],
            annot=True,
            fmt='d',
            cmap='Blues'
        )
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.title(f'Confusion Matrix - Fold {fold+1}')
        plt.savefig(os.path.join(fold_save_dir, 'confusion_matrix.png'))
        plt.close()

        # QUAN TRỌNG: Xóa mô hình khỏi GPU và làm sạch bộ nhớ
        print(f"Clearing model from memory for fold {fold+1}...")
        del model, optimizer, criterion, fold_train_dataloader, fold_val_dataloader
        del fold_train_dataset, fold_val_dataset, fold_tokenizer
        torch.cuda.empty_cache()  # Giải phóng bộ nhớ GPU
        import gc
        gc.collect()  # Chạy garbage collector

    # Tính toán kết quả trung bình
    mean_accuracy = np.mean(cv_accuracies)
    mean_loss = np.mean(cv_losses)
    mean_f1_weighted = np.mean(cv_f1_weighted)
    mean_f1_macro = np.mean(cv_f1_macro)

    std_accuracy = np.std(cv_accuracies)
    std_loss = np.std(cv_losses)
    std_f1_weighted = np.std(cv_f1_weighted)
    std_f1_macro = np.std(cv_f1_macro)

    # In kết quả tổng hợp
    print("\n" + "=" * 50)
    print(f"Cross-Validation Results ({n_folds} folds):")
    print(f"Mean Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}")
    print(f"Mean Loss: {mean_loss:.4f} ± {std_loss:.4f}")
    print(f"Mean F1 Weighted: {mean_f1_weighted:.4f} ± {std_f1_weighted:.4f}")
    print(f"Mean F1 Macro: {mean_f1_macro:.4f} ± {std_f1_macro:.4f}")
    print("=" * 50)

    # Vẽ kết quả cross validation
    plt.figure(figsize=(12, 8))

    # Plot accuracies
    plt.subplot(2, 2, 1)
    plt.bar(range(1, n_folds+1), cv_accuracies)
    plt.axhline(y=mean_accuracy, color='r', linestyle='-', label=f'Mean: {mean_accuracy:.4f}')
    plt.fill_between(
        [0.5, n_folds+0.5],
        [mean_accuracy - std_accuracy] * 2,
        [mean_accuracy + std_accuracy] * 2,
        alpha=0.2, color='r'
    )
    plt.xlabel('Fold')
    plt.ylabel('Accuracy')
    plt.title('Accuracy per Fold')
    plt.legend()
    plt.xticks(range(1, n_folds+1))

    # Plot losses
    plt.subplot(2, 2, 2)
    plt.bar(range(1, n_folds+1), cv_losses)
    plt.axhline(y=mean_loss, color='r', linestyle='-', label=f'Mean: {mean_loss:.4f}')
    plt.fill_between(
        [0.5, n_folds+0.5],
        [mean_loss - std_loss] * 2,
        [mean_loss + std_loss] * 2,
        alpha=0.2, color='r'
    )
    plt.xlabel('Fold')
    plt.ylabel('Loss')
    plt.title('Loss per Fold')
    plt.legend()
    plt.xticks(range(1, n_folds+1))

    # Plot F1 Weighted
    plt.subplot(2, 2, 3)
    plt.bar(range(1, n_folds+1), cv_f1_weighted)
    plt.axhline(y=mean_f1_weighted, color='r', linestyle='-', label=f'Mean: {mean_f1_weighted:.4f}')
    plt.fill_between(
        [0.5, n_folds+0.5],
        [mean_f1_weighted - std_f1_weighted] * 2,
        [mean_f1_weighted + std_f1_weighted] * 2,
        alpha=0.2, color='r'
    )
    plt.xlabel('Fold')
    plt.ylabel('F1 Weighted')
    plt.title('F1 Weighted per Fold')
    plt.legend()
    plt.xticks(range(1, n_folds+1))

    # Plot F1 Macro
    plt.subplot(2, 2, 4)
    plt.bar(range(1, n_folds+1), cv_f1_macro)
    plt.axhline(y=mean_f1_macro, color='r', linestyle='-', label=f'Mean: {mean_f1_macro:.4f}')
    plt.fill_between(
        [0.5, n_folds+0.5],
        [mean_f1_macro - std_f1_macro] * 2,
        [mean_f1_macro + std_f1_macro] * 2,
        alpha=0.2, color='r'
    )
    plt.xlabel('Fold')
    plt.ylabel('F1 Macro')
    plt.title('F1 Macro per Fold')
    plt.legend()
    plt.xticks(range(1, n_folds+1))

    plt.tight_layout()
    plt.savefig(os.path.join(save_model_dir, 'cross_validation_results.png'))
    plt.show()

    # Tạo log file tổng hợp cho tất cả các fold
    log_file_path = os.path.join(save_model_dir, f'metrics_summary.log')
    logger = setup_logger(log_file_path)
    logger.info(f"Cross-Validation Summary ({n_folds} folds):")
    logger.info(f"Mean Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}")
    logger.info(f"Mean Loss: {mean_loss:.4f} ± {std_loss:.4f}")
    logger.info(f"Mean F1 Weighted: {mean_f1_weighted:.4f} ± {std_f1_weighted:.4f}")
    logger.info(f"Mean F1 Macro: {mean_f1_macro:.4f} ± {std_f1_macro:.4f}")

    # Tạo kết quả trả về
    cv_results = {
        'mean_accuracy': mean_accuracy,
        'mean_loss': mean_loss,
        'mean_f1_weighted': mean_f1_weighted,
        'mean_f1_macro': mean_f1_macro,
        'std_accuracy': std_accuracy,
        'std_loss': std_loss,
        'std_f1_weighted': std_f1_weighted,
        'std_f1_macro': std_f1_macro,
        'accuracies': cv_accuracies,
        'losses': cv_losses,
        'f1_weighted': cv_f1_weighted,
        'f1_macro': cv_f1_macro,
        'metrics_per_fold': cv_metrics_per_fold
    }

    return cv_results

# # Gộp dữ liệu train và valid để thực hiện cross-validation
# combined_df = pd.concat([train_df, valid_df], ignore_index=True)

# Thực hiện cross-validation
print("Performing 5-fold cross-validation...")
cv_save_dir = os.path.join(save_path, 'cross_validation')
os.makedirs(cv_save_dir, exist_ok=True)

# Thực hiện cross-validation chỉ với tập train
cv_results = perform_cross_validation(
    train_df=train_df,  # CHỈ sử dụng tập train
    n_folds=5,
    num_epochs=10,
    batch_size=1024,
    save_model_dir=cv_save_dir
)

